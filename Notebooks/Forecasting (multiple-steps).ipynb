{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from FileManager.dataManager import dataManager\n",
    "from AnalyzeTools.prepare import model_eval, pathForSavingModels\n",
    "from AnalyzeTools.preprocess import preprocessData\n",
    "from AnalyzeTools.superModels import DEEPAR, TFT, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = './Models'\n",
    "product_object = json.load(open(\"./File information.json\", \"r\", encoding='utf8'))\n",
    "\n",
    "all_experiments= []\n",
    "for product in product_object.keys():\n",
    "    for raw_file_name in  product_object[product].keys():\n",
    "        for product_type in product_object[product][raw_file_name]['product_types']:\n",
    "            for target in product_object[product][raw_file_name]['targets']:\n",
    "                all_experiments.append([product, raw_file_name, product_type, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "experiment = all_experiments[n]\n",
    "product, raw_file_name, product_type, target = experiment\n",
    "print(f\"Product: {product}\\nRaw file name: {raw_file_name}\\nProduct_type: {product_type}\\ntarget: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, product_and_product_type, product_attribute = dataManager(raw_file_name, product, product_type, target)\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No data!\")\n",
    "\n",
    "df, input_features = preprocessData(df, 'date', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "data['time_idx'] = range(len(data))\n",
    "data['group'] = product\n",
    "\n",
    "training_cutoff = floor(len(data) * 0.8)\n",
    "\n",
    "max_prediction_length = 14\n",
    "max_encoder_length = 60 # 7, 14, 30, 60, 120\n",
    "batch_size = 64\n",
    "\n",
    "group = ['group']\n",
    "# time_varying_known_categoricals = ['month', 'week']\n",
    "time_varying_known_categoricals = []\n",
    "time_varying_unknown_categoricals = []\n",
    "time_varying_known_reals = ['time_idx']\n",
    "time_varying_unknown_reals = input_features + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, torch, shutil\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(123)\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, DeepAR, RecurrentNetwork, GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAPE, NormalDistributionLoss, QuantileLoss, SMAPE\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from AnalyzeTools.prepare import retriveBestModelPath, save_best_params\n",
    "from AnalyzeTools.paramsTuner import optimize_hyperparameters_for_RNN, optimize_hyperparameters_for_DeepAR\n",
    "\n",
    "def DEEPAR(\n",
    "    data, \n",
    "    training_cutoff, \n",
    "    target, \n",
    "    group, \n",
    "    max_encoder_length, \n",
    "    max_prediction_length, \n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals, \n",
    "    time_varying_known_reals, \n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    training_params={},\n",
    "):\n",
    "    best_model_path = retriveBestModelPath(saving_dir)\n",
    "    data[time_varying_known_categoricals] = data[time_varying_known_categoricals].astype(str).astype(\"category\")\n",
    "    max_epochs = training_params.get('max_epochs') if training_params.get('max_epochs') else 100\n",
    "    n_trials = training_params.get('n_trials') if training_params.get('n_trials') else 30\n",
    "\n",
    "    data[time_varying_known_categoricals] = data[time_varying_known_categoricals].astype(str).astype(\"category\")\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x: x.time_idx <= training_cutoff],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target,\n",
    "        group_ids=group,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "        time_varying_unknown_categoricals=time_varying_unknown_categoricals,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=[target],\n",
    "        target_normalizer=GroupNormalizer(groups=group),\n",
    "        categorical_encoders={\n",
    "            \"month\": NaNLabelEncoder().fit(data.month),\n",
    "            \"week\": NaNLabelEncoder().fit(data.week)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training, \n",
    "        data, \n",
    "        min_prediction_idx=training.index.time.max() + 1 + max_encoder_length - 1,\n",
    "        stop_randomization=True\n",
    "    )\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    if not best_model_path:\n",
    "\n",
    "        # create study\n",
    "        study = optimize_hyperparameters_for_DeepAR(\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            model_path=saving_dir,\n",
    "            n_trials=n_trials,\n",
    "            max_epochs=max_epochs,\n",
    "            gradient_clip_val_range=(0.01, 1.0),\n",
    "            hidden_size_range=(8, 128),\n",
    "            learning_rate_range=(0.001, 0.1),\n",
    "            dropout_range=(0.1, 0.3),\n",
    "            trainer_kwargs=dict(limit_train_batches=30),\n",
    "            reduce_on_plateau_patience=4,\n",
    "            use_learning_rate_finder=False,\n",
    "            log_dir=saving_dir\n",
    "        )\n",
    "\n",
    "        best_params = study.best_params\n",
    "        shutil.rmtree(saving_dir)\n",
    "\n",
    "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", verbose=False, mode=\"min\")\n",
    "        lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            gpus=0,\n",
    "            enable_model_summary=False,\n",
    "            callbacks=[lr_logger, early_stop_callback],\n",
    "            log_every_n_steps=10,\n",
    "            check_val_every_n_epoch=3, \n",
    "            default_root_dir=saving_dir,\n",
    "        )\n",
    "\n",
    "        deep_ar = DeepAR.from_dataset(\n",
    "            training,\n",
    "            hidden_size=best_params.get(\"hidden_size\"),\n",
    "            rnn_layers=best_params.get('rnn_layers'),\n",
    "            learning_rate=best_params.get('learning_rate'),\n",
    "            dropout=best_params.get(\"dropout\"),\n",
    "            loss=NormalDistributionLoss(),\n",
    "            log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "            log_val_interval=3,\n",
    "        )\n",
    "        # print(f\"Number of parameters in network: {deep_ar.size()/1e3:.1f}k\")\n",
    "\n",
    "        trainer.fit(\n",
    "            deep_ar,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=val_dataloader,\n",
    "        )\n",
    "\n",
    "        save_best_params(best_params, f\"{saving_dir}/Best_DeepAR.json\")\n",
    "\n",
    "    best_model_path = retriveBestModelPath(saving_dir)\n",
    "    deep_ar = DeepAR.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    return deep_ar, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_searchCV_params = {\n",
    "    'base_dir': params_path,\n",
    "    'product_and_product_type': product_and_product_type,\n",
    "    'attribute': product_attribute,\n",
    "    'raw': raw_file_name,\n",
    "    'predict_type': 'multiple',\n",
    "    'period': 'Day',\n",
    "    'step': max_prediction_length,\n",
    "    'save': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nDeepAR\")\n",
    "# training_params = {'max_epochs': 10, 'n_trials': 2}\n",
    "# saving_dir = pathForSavingModels('DeepAR', **dl_searchCV_params)\n",
    "# deep_ar, val_dataloader = DEEPAR(\n",
    "#     data,\n",
    "#     training_cutoff,\n",
    "#     target,\n",
    "#     group,\n",
    "#     max_encoder_length,\n",
    "#     max_prediction_length,\n",
    "#     time_varying_known_categoricals,\n",
    "#     time_varying_unknown_categoricals,\n",
    "#     time_varying_known_reals,\n",
    "#     batch_size,\n",
    "#     saving_dir,\n",
    "#     training_params\n",
    "# )\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# deepar_predictions = deep_ar.predict(val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTFT\")\n",
    "training_params = {'max_epochs': 10, 'n_trials': 1}\n",
    "saving_dir = pathForSavingModels('TFT', **dl_searchCV_params)\n",
    "tft, val_dataloader = TFT(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    time_varying_unknown_reals,\n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    training_params,\n",
    "    predict_type='multiple'\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "tft_predictions = tft.predict(val_dataloader)\n",
    "\n",
    "# model_eval(actuals, tft_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = model.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions, x = model.predict(val_dataloader, mode='raw', return_x=True, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_predictions, x = model.predict(val_dataloader, mode='quantiles', return_x=True, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = validation.x_to_index(x)[\"series\"]\n",
    "for idx in range(20):  # plot 10 examples\n",
    "    model.plot_prediction(x, predictions, idx=idx, add_loss_to_title=True)\n",
    "    # plt.suptitle(f\"Series: {series.iloc[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for idx in range(1, 2000, max_prediction_length):  # plot 10 examples\n",
    "    if cnt >= 50:\n",
    "        break\n",
    "    try:\n",
    "        model.plot_prediction(x, predictions, idx=idx, add_loss_to_title=True)\n",
    "        cnt += 1\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = model.interpret_output(predictions, reduction=\"sum\")\n",
    "model.plot_interpretation(interpretation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f774c96c8c34c53ecd4c73b34542f198e825b7806220478caf5e39d6877a780"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
