{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zyf13\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************  2/63  **************************************************\n",
      "There are too few features in the data. The raw data features will be used.\n",
      "Too few features to filter!\n",
      "\n",
      "-->Final features:\n",
      "  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, json\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from FileManager.dataManager import dataManager\n",
    "\n",
    "from AnalyzeTools.models import autoregressive_integrated_moving_average, linear_regression, support_vector_regression, random_forest, gradient_boosting\n",
    "from AnalyzeTools.prepare import data_split, model_eval, pathForSavingModels\n",
    "from AnalyzeTools.preprocess import preprocessData\n",
    "from AnalyzeTools.superModels import DEEPAR, TFT, RNN\n",
    "\n",
    "params_path = './Models/single'\n",
    "train_size = 0.8\n",
    "product_object = json.load(open(\"./File information.json\", \"r\", encoding='utf8'))\n",
    "\n",
    "all_experiments= []\n",
    "for product in product_object.keys():\n",
    "    for raw_file_name in  product_object[product].keys():\n",
    "        for product_type in product_object[product][raw_file_name]['product_types']:\n",
    "            for target in product_object[product][raw_file_name]['targets']:\n",
    "                all_experiments.append([product, raw_file_name, product_type, target])\n",
    "i = 2\n",
    "experiment = all_experiments[2]\n",
    "print(\"*\"*50 + f\"  {i}/{len(all_experiments)}  \"  + \"*\"*50)\n",
    "product, raw_file_name, product_type, target = experiment\n",
    "df, product_and_product_type, product_attribute = dataManager(raw_file_name, product, product_type, target)\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No data!\")\n",
    "\n",
    "df, input_features = preprocessData(df, 'date', target)\n",
    "predictions_x_axis = df['date'][floor(len(df)*train_size):].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-26 22:58:38,068]\u001b[0m A new study created in memory with name: no-name-c993b3b0-3d3e-42f2-9afa-343888ba0814\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (997, 1) y_train: (997,) X_test: (250, 1) y_test: (250,)\n",
      "\n",
      "Linear Regression\n",
      "\n",
      "Support Vector Regression\n",
      "--> Start searching best parameters!\n",
      "\n",
      "Best parameter for SVR is:\n",
      "  {'C': 1, 'gamma': 0.01, 'epsilon': 0.05, 'kernel': 'rbf'}\n",
      "\n",
      "Random Forest\n",
      "--> Start searching best parameters!\n",
      "\n",
      "Best parameter for Random forest is:\n",
      "  {'n_estimators': 270, 'max_depth': 1, 'max_features': 1, 'min_samples_leaf': 9, 'min_samples_split': 5}\n",
      "\n",
      "Gradient Boosting\n",
      "--> Start searching best parameters!\n",
      "\n",
      "Best parameter for Gradient Boosting is:\n",
      "  {'n_estimators': 110, 'max_depth': 1, 'max_features': 1, 'min_samples_leaf': 3, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare dataset for statistics and Macnhine models\n",
    "ml_split_params = {'Model': 'ML', 'Future': 1}\n",
    "X_train, X_test, y_train, y_test, input_scaler, output_scaler = data_split(df, input_features, output=target, train_size=train_size, scaling=True, **ml_split_params)\n",
    "\n",
    "''' Input data into models and Evaluate model results '''\n",
    "ml_searchCV_params = {\n",
    "    'base_dir': params_path,\n",
    "    'product': product_and_product_type,\n",
    "    'attribute': product_attribute,\n",
    "    'raw': raw_file_name,\n",
    "    'save': True\n",
    "}\n",
    "# stdout = False\n",
    "# vis = False\n",
    "\n",
    "# print(\"\\nARIMA\")\n",
    "# arima_predictions = autoregressive_integrated_moving_average(y_train, y_test)\n",
    "# model_eval(y_test, arima_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})\n",
    "\n",
    "print(\"\\nLinear Regression\")\n",
    "lr, _ = linear_regression(X_train, y_train)\n",
    "# lr_predictions = lr.predict(X_test)\n",
    "# model_eval(y_test, lr_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})\n",
    "\n",
    "print(\"\\nSupport Vector Regression\")\n",
    "svr, _ = support_vector_regression(X_train, y_train, search=True, **ml_searchCV_params)\n",
    "# svr_predictions = svr.predict(X_test)\n",
    "# model_eval(y_test, svr_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})\n",
    "\n",
    "print(\"\\nRandom Forest\")\n",
    "rf, _ = random_forest(X_train, y_train, search=True, **ml_searchCV_params, **{'scaler': output_scaler})\n",
    "# rf_predictions = rf.predict(X_test)\n",
    "# model_eval(y_test, rf_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})\n",
    "\n",
    "print(\"\\nGradient Boosting\")\n",
    "gb, _ = gradient_boosting(X_train, y_train, search=True, **ml_searchCV_params)\n",
    "# gb_predictions = gb.predict(X_test)\n",
    "# model_eval(y_test, gb_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "data['time_idx'] = range(len(data))\n",
    "data['group'] = product\n",
    "\n",
    "training_cutoff = floor(len(data) * train_size)\n",
    "\n",
    "max_prediction_length = 1\n",
    "max_encoder_length = 30 # 7, 14, 30, 60, 120\n",
    "batch_size = 64\n",
    "\n",
    "group = ['group']\n",
    "time_varying_known_categoricals = ['month', 'week']\n",
    "time_varying_unknown_categoricals = []\n",
    "time_varying_known_reals = ['time_idx']\n",
    "time_varying_unknown_reals = input_features + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings, torch, shutil\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# pl.seed_everything(123)\n",
    "\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, DeepAR, RecurrentNetwork, GroupNormalizer\n",
    "# from pytorch_forecasting.metrics import MAPE, NormalDistributionLoss, QuantileLoss, SMAPE\n",
    "\n",
    "# from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "# from AnalyzeTools.prepare import retriveBestModelPath\n",
    "\n",
    "# def RNN(\n",
    "#     data, \n",
    "#     training_cutoff, \n",
    "#     target, \n",
    "#     group, \n",
    "#     max_encoder_length, \n",
    "#     max_prediction_length, \n",
    "#     time_varying_known_categoricals,\n",
    "#     time_varying_unknown_categoricals, \n",
    "#     time_varying_known_reals, \n",
    "#     batch_size,\n",
    "#     saving_dir,\n",
    "#     cell='LSTM'\n",
    "# ):\n",
    "#     best_model_path = retriveBestModelPath(saving_dir)\n",
    "\n",
    "#     data[time_varying_known_categoricals] = data[time_varying_known_categoricals].astype(str).astype(\"category\")\n",
    "#     training = TimeSeriesDataSet(\n",
    "#         data[lambda x: x.time_idx <= training_cutoff],\n",
    "#         time_idx=\"time_idx\",\n",
    "#         target=target,\n",
    "#         group_ids=group,\n",
    "#         max_encoder_length=max_encoder_length,\n",
    "#         max_prediction_length=max_prediction_length,\n",
    "#         time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "#         time_varying_unknown_categoricals=time_varying_unknown_categoricals,\n",
    "#         time_varying_known_reals=time_varying_known_reals,\n",
    "#         time_varying_unknown_reals=[target],\n",
    "#     )\n",
    "\n",
    "#     validation = TimeSeriesDataSet.from_dataset(\n",
    "#         training, \n",
    "#         data, \n",
    "#         min_prediction_idx=training.index.time.max() + 1 + max_encoder_length - 1,\n",
    "#         stop_randomization=True\n",
    "#     )\n",
    "\n",
    "#     train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "#     val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "#     print(best_model_path)\n",
    "#     if not best_model_path:\n",
    "#         early_stop_callback = EarlyStopping(monitor=\"val_loss\", verbose=False, mode=\"min\")\n",
    "#         lr_logger = LearningRateMonitor()\n",
    "\n",
    "#         trainer = pl.Trainer(\n",
    "#             max_epochs=100,\n",
    "#             gpus=0,\n",
    "#             weights_summary='top',\n",
    "#             callbacks=[lr_logger, early_stop_callback],\n",
    "#             log_every_n_steps=10,\n",
    "#             check_val_every_n_epoch=3,\n",
    "#             default_root_dir=saving_dir,\n",
    "#         )\n",
    "\n",
    "#         model = RecurrentNetwork.from_dataset(\n",
    "#             training,\n",
    "#             cell_type=cell,\n",
    "#             hidden_size=128,\n",
    "#             rnn_layers=1,\n",
    "#             dropout=0.1,\n",
    "#             output_size=1,\n",
    "#             loss=MAPE(),\n",
    "#             log_interval=10\n",
    "#         )\n",
    "\n",
    "#         trainer.fit(\n",
    "#             model,\n",
    "#             train_dataloaders=train_dataloader,\n",
    "#             val_dataloaders=val_dataloader\n",
    "#         )\n",
    "    \n",
    "#     best_model_path = retriveBestModelPath(saving_dir)\n",
    "#     best_model = RecurrentNetwork.load_from_checkpoint(best_model_path, cell_type=cell)\n",
    "\n",
    "#     return best_model, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Agriculture prediction\\\\Notebooks/Models/single/pork(CTSED_CODE=4301)/소매가격/(중)축산유통정보 - 소비자가격/LSTM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathForSavingModels(product_and_product_type, product_attribute, raw_file_name, 'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: d:\\Agriculture prediction\\Notebooks\\Models\\single\\pork\\pork(CTSED_CODE=4301)\\(중)축산유통정보 - 소비자가격\\LSTM\\lightning_logs\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | loss             | MAPE           | 0     \n",
      "1 | logging_metrics  | ModuleList     | 0     \n",
      "2 | embeddings       | MultiEmbedding | 867   \n",
      "3 | rnn              | LSTM           | 78.3 K\n",
      "4 | output_projector | Linear         | 129   \n",
      "----------------------------------------------------\n",
      "79.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "79.3 K    Total params\n",
      "0.317     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM\n",
      "False\n",
      "Epoch 14: 100%|██████████| 19/19 [00:02<00:00,  9.23it/s, loss=0.116, v_num=0, train_loss_step=0.105, train_loss_epoch=0.117, val_loss=0.0897] \n",
      "\n",
      "GRU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: d:\\Agriculture prediction\\Notebooks\\Models\\single\\pork\\pork(CTSED_CODE=4301)\\(중)축산유통정보 - 소비자가격\\GRU\\lightning_logs\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | loss             | MAPE           | 0     \n",
      "1 | logging_metrics  | ModuleList     | 0     \n",
      "2 | embeddings       | MultiEmbedding | 867   \n",
      "3 | rnn              | GRU            | 58.8 K\n",
      "4 | output_projector | Linear         | 129   \n",
      "----------------------------------------------------\n",
      "59.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "59.7 K    Total params\n",
      "0.239     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 19/19 [00:01<00:00, 10.76it/s, loss=0.117, v_num=0, train_loss_step=0.122, train_loss_epoch=0.116, val_loss=0.0903] \n",
      "\n",
      "DeepAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: d:\\Agriculture prediction\\Notebooks\\Models\\single\\pork\\pork(CTSED_CODE=4301)\\(중)축산유통정보 - 소비자가격\\DEEPAR\\lightning_logs\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 867   \n",
      "3 | rnn                    | LSTM                   | 211 K \n",
      "4 | distribution_projector | Linear                 | 258   \n",
      "------------------------------------------------------------------\n",
      "212 K     Trainable params\n",
      "0         Non-trainable params\n",
      "212 K     Total params\n",
      "0.850     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 0/15 [00:00<?, ?it/s, loss=8.59, v_num=0, train_loss_step=8.590, train_loss_epoch=8.590, val_loss=8.550]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-26 22:56:11,311]\u001b[0m A new study created in memory with name: no-name-a761f040-867d-424c-85c9-25179dde0017\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLSTM\")\n",
    "lstm, val_dataloader = RNN(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    pathForSavingModels(product, product_and_product_type, raw_file_name, 'LSTM'),\n",
    "    'LSTM'\n",
    ")\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# lstm_predictions = lstm.predict(val_dataloader)\n",
    "# model_eval(actuals, lstm_predictions, predictions_x_axis, stdout=True, vis=True)\n",
    "\n",
    "print(\"\\nGRU\")\n",
    "gru, val_dataloader = RNN(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    pathForSavingModels(product, product_and_product_type, raw_file_name, 'GRU'),\n",
    "    'GRU'\n",
    ")\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# gru_predictions = gru.predict(val_dataloader)\n",
    "# model_eval(actuals, gru_predictions, predictions_x_axis, stdout=True, vis=True)\n",
    "\n",
    "print(\"\\nDeepAR\")\n",
    "deep_ar, val_dataloader = DEEPAR(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    pathForSavingModels(product, product_and_product_type, raw_file_name, 'DEEPAR'),\n",
    ")\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# deepar_predictions = deep_ar.predict(val_dataloader)\n",
    "\n",
    "# model_eval(actuals, deepar_predictions, predictions_x_axis, stdout=True, vis=True)\n",
    "\n",
    "print(\"\\nTFT\")\n",
    "tft, val_dataloader = TFT(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    time_varying_unknown_reals,\n",
    "    batch_size,\n",
    "    pathForSavingModels(product, product_and_product_type, raw_file_name, 'TFT'),\n",
    ")\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# tft_predictions = tft.predict(val_dataloader)\n",
    "# model_eval(actuals, tft_predictions, predictions_x_axis, stdout=True, vis=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
