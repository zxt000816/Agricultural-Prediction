{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from FileManager.dataManager import dataManager\n",
    "from AnalyzeTools.models import autoregressive_integrated_moving_average, linear_regression, support_vector_regression, random_forest, gradient_boosting\n",
    "from AnalyzeTools.prepare import data_split, model_eval, pathForSavingModels\n",
    "from AnalyzeTools.preprocess import preprocessData, removeOutliers\n",
    "from AnalyzeTools.superModels import DEEPAR, TFT, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 'Day'\n",
    "future_step = 1\n",
    "params_path = f'./Models'\n",
    "\n",
    "product_object = json.load(open(\"./File information.json\", \"r\", encoding='utf8'))\n",
    "\n",
    "all_experiments= []\n",
    "for product in product_object.keys():\n",
    "    for raw_file_name in  product_object[product].keys():\n",
    "        for product_type in product_object[product][raw_file_name]['product_types']:\n",
    "            for target in product_object[product][raw_file_name]['targets']:\n",
    "                all_experiments.append([product, raw_file_name, product_type, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "experiment = all_experiments[n]\n",
    "product, raw_file_name, product_type, target = experiment\n",
    "print(f\"Product: {product}\\nRaw file name: {raw_file_name}\\nProduct_type: {product_type}\\ntarget: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, product_and_product_type, product_attribute = dataManager(raw_file_name, product, product_type, target)\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No data!\")\n",
    "\n",
    "df, input_features = preprocessData(df, 'date', target, 'Day')\n",
    "test_size = 0.2\n",
    "\n",
    "df = removeOutliers(df, test_size, target)\n",
    "\n",
    "train_x_axis = df['date'][:-1*floor(len(df) * test_size)].values if type(test_size) == float else df['date'][:-1*test_size]\n",
    "predictions_x_axis = df['date'][-1*floor(len(df) * test_size):].values if type(test_size) == float else df['date'][-1*test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for statistics and Macnhine models\n",
    "ml_split_params = {'Model': 'ML', 'Future': future_step}\n",
    "X_train, X_test, y_train, y_test, input_scaler, output_scaler = data_split(df, input_features, target, test_size, scaling=True, **ml_split_params)\n",
    "\n",
    "''' Input data into models and Evaluate model results '''\n",
    "ml_searchCV_params = {\n",
    "    'base_dir': params_path,\n",
    "    'product_and_product_type': product_and_product_type,\n",
    "    'attribute': product_attribute,\n",
    "    'raw': raw_file_name,\n",
    "    'predict_type': 'single',\n",
    "    'period': 'Day',\n",
    "    'step': future_step,\n",
    "    'save': True\n",
    "}\n",
    "stdout = True\n",
    "vis = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nARIMA\")\n",
    "# arima_predictions = autoregressive_integrated_moving_average(y_train, y_test)\n",
    "# model_eval(y_test, arima_predictions, stdout=stdout, vis=vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLinear Regression\")\n",
    "lr, _ = linear_regression(X_train, y_train)\n",
    "lr_predictions = lr.predict(X_test)\n",
    "model_eval(y_test, lr_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSupport Vector Regression\")\n",
    "svr, _ = support_vector_regression(X_train, y_train, search=True, **ml_searchCV_params)\n",
    "svr_predictions = svr.predict(X_test)\n",
    "model_eval(y_test, svr_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRandom Forest\")\n",
    "rf, _ = random_forest(X_train, y_train, search=True, **ml_searchCV_params)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "model_eval(y_test, rf_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGradient Boosting\")\n",
    "gb, _ = gradient_boosting(X_train, y_train, search=True, **ml_searchCV_params)\n",
    "gb_predictions = gb.predict(X_test)\n",
    "model_eval(y_test, gb_predictions, predictions_x_axis, stdout=stdout, vis=vis, **{'scaler': output_scaler})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "# if future_step != 1:\n",
    "#     dl_target = f'{target}_lead_{future_step}'\n",
    "#     data[dl_target] = data[target].shift(-1 * (future_step - 1))\n",
    "#     data = data[:-1 * (future_step - 1)]\n",
    "\n",
    "data['time_idx'] = range(len(data))\n",
    "data['group'] = product\n",
    "\n",
    "training_cutoff = floor(len(data) * (1-test_size)) if type(test_size) == float else len(data) - test_size\n",
    "\n",
    "max_prediction_length = 1 # In the case of point forecasting, this parameter is 1 by default\n",
    "max_encoder_length = 30 # 7, 14, 30, 60, 120\n",
    "batch_size = 64\n",
    "\n",
    "group = ['group']\n",
    "time_varying_known_categoricals = ['month', 'week']\n",
    "time_varying_unknown_categoricals = []\n",
    "time_varying_known_reals = ['time_idx']\n",
    "time_varying_unknown_reals = input_features + [target]\n",
    "\n",
    "# if future_step != 1:\n",
    "#     time_varying_unknown_reals = input_features + [target, dl_target]\n",
    "#     target_arg = dl_target\n",
    "# else:\n",
    "#     time_varying_unknown_reals = input_features + [target]\n",
    "#     target_arg = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_searchCV_params = {\n",
    "    'base_dir': params_path,\n",
    "    'product_and_product_type': product_and_product_type,\n",
    "    'attribute': product_attribute,\n",
    "    'raw': raw_file_name,\n",
    "    'predict_type': 'single',\n",
    "    'period': 'Day',\n",
    "    'step': future_step,\n",
    "    'save': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLSTM\")\n",
    "training_params = {'max_epochs': 100, 'n_trials': 30}\n",
    "saving_dir = pathForSavingModels('LSTM', **dl_searchCV_params)\n",
    "lstm, val_dataloader = RNN(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    'LSTM',\n",
    "    training_params,\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "lstm_predictions = lstm.predict(val_dataloader)\n",
    "model_eval(actuals, lstm_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGRU\")\n",
    "training_params = {'max_epochs': 100, 'n_trials': 30}\n",
    "saving_dir = pathForSavingModels('GRU', **dl_searchCV_params)\n",
    "gru, val_dataloader = RNN(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    'GRU',\n",
    "    training_params\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "gru_predictions = gru.predict(val_dataloader)\n",
    "\n",
    "model_eval(actuals, gru_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDeepAR\")\n",
    "training_params = {'max_epochs': 100, 'n_trials': 30}\n",
    "saving_dir = pathForSavingModels('DeepAR', **dl_searchCV_params)\n",
    "deep_ar, val_dataloader = DEEPAR(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    training_params\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "deepar_predictions = deep_ar.predict(val_dataloader)\n",
    "\n",
    "model_eval(actuals, deepar_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTFT\")\n",
    "training_params = {'max_epochs': 50, 'n_trials': 10}\n",
    "saving_dir = pathForSavingModels('TFT', **dl_searchCV_params)\n",
    "tft, val_dataloader = TFT(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    time_varying_unknown_reals,\n",
    "    batch_size,\n",
    "    saving_dir,\n",
    "    training_params\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "tft_predictions = tft.predict(val_dataloader)\n",
    "\n",
    "model_eval(actuals, tft_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f774c96c8c34c53ecd4c73b34542f198e825b7806220478caf5e39d6877a780"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
