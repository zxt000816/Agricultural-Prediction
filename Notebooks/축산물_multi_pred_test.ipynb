{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Hyperparameters can be efficiently tuned with `optuna <https://optuna.readthedocs.io/>`_.\n",
    "# \"\"\"\n",
    "# import copy\n",
    "# import logging\n",
    "# import os\n",
    "# from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "# import numpy as np\n",
    "# import optuna\n",
    "# from optuna.integration import PyTorchLightningPruningCallback, TensorBoardCallback\n",
    "# import optuna.logging\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import Callback\n",
    "# from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# import statsmodels.api as sm\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from pytorch_forecasting import TemporalFusionTransformer, RecurrentNetwork\n",
    "# from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "# from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# optuna_logger = logging.getLogger(\"optuna\")\n",
    "\n",
    "# class MetricsCallback(Callback):\n",
    "#     \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.metrics = []\n",
    "\n",
    "#     def on_validation_end(self, trainer, pl_module):\n",
    "#         self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_hyperparameters_for_rnn(\n",
    "#     train_dataloaders: DataLoader,\n",
    "#     val_dataloaders: DataLoader,\n",
    "#     model_path: str,\n",
    "#     max_epochs: int = 20,\n",
    "#     n_trials: int = 100,\n",
    "#     timeout: float = 3600 * 8.0,  # 8 hours\n",
    "#     gradient_clip_val_range: Tuple[float, float] = (0.01, 100.0),\n",
    "#     hidden_size_range: Tuple[int, int] = (16, 265),\n",
    "#     # hidden_continuous_size_range: Tuple[int, int] = (8, 64),\n",
    "#     # attention_head_size_range: Tuple[int, int] = (1, 4),\n",
    "#     dropout_range: Tuple[float, float] = (0.1, 0.3),\n",
    "#     learning_rate_range: Tuple[float, float] = (1e-5, 1.0),\n",
    "#     use_learning_rate_finder: bool = True,\n",
    "#     trainer_kwargs: Dict[str, Any] = {},\n",
    "#     log_dir: str = \"lightning_logs\",\n",
    "#     study: optuna.Study = None,\n",
    "#     verbose: Union[int, bool] = None,\n",
    "#     pruner: optuna.pruners.BasePruner = optuna.pruners.SuccessiveHalvingPruner(),\n",
    "#     **kwargs,\n",
    "# ) -> optuna.Study:\n",
    "\n",
    "#     assert isinstance(train_dataloaders.dataset, TimeSeriesDataSet) and isinstance(\n",
    "#         val_dataloaders.dataset, TimeSeriesDataSet\n",
    "#     ), \"dataloaders must be built from timeseriesdataset\"\n",
    "\n",
    "#     logging_level = {\n",
    "#         None: optuna.logging.get_verbosity(),\n",
    "#         0: optuna.logging.WARNING,\n",
    "#         1: optuna.logging.INFO,\n",
    "#         2: optuna.logging.DEBUG,\n",
    "#     }\n",
    "#     optuna_verbose = logging_level[verbose]\n",
    "#     optuna.logging.set_verbosity(optuna_verbose)\n",
    "\n",
    "#     loss = kwargs.get(\n",
    "#         \"loss\", QuantileLoss()\n",
    "#     )  # need a deepcopy of loss as it will otherwise propagate from one trial to the next\n",
    "\n",
    "#     # create objective function\n",
    "#     def objective(trial: optuna.Trial) -> float:\n",
    "#         # Filenames for each trial must be made unique in order to access each checkpoint.\n",
    "#         checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#             dirpath=os.path.join(model_path, \"trial_{}\".format(trial.number)), filename=\"{epoch}\", monitor=\"val_loss\"\n",
    "#         )\n",
    "\n",
    "#         # The default logger in PyTorch Lightning writes to event files to be consumed by\n",
    "#         # TensorBoard. We don't use any logger here as it requires us to implement several abstract\n",
    "#         # methods. Instead we setup a simple callback, that saves metrics from each validation step.\n",
    "#         metrics_callback = MetricsCallback()\n",
    "#         learning_rate_callback = LearningRateMonitor()\n",
    "#         logger = TensorBoardLogger(log_dir, name=\"optuna\", version=trial.number)\n",
    "#         gradient_clip_val = trial.suggest_loguniform(\"gradient_clip_val\", *gradient_clip_val_range)\n",
    "#         default_trainer_kwargs = dict(\n",
    "#             gpus=[0] if torch.cuda.is_available() else None,\n",
    "#             max_epochs=max_epochs,\n",
    "#             gradient_clip_val=gradient_clip_val,\n",
    "#             callbacks=[\n",
    "#                 metrics_callback,\n",
    "#                 learning_rate_callback,\n",
    "#                 checkpoint_callback,\n",
    "#                 PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n",
    "#             ],\n",
    "#             logger=logger,\n",
    "#             enable_progress_bar=optuna_verbose < optuna.logging.INFO,\n",
    "#             weights_summary=[None, \"top\"][optuna_verbose < optuna.logging.INFO],\n",
    "#         )\n",
    "#         default_trainer_kwargs.update(trainer_kwargs)\n",
    "#         trainer = pl.Trainer(\n",
    "#             **default_trainer_kwargs,\n",
    "#         )\n",
    "\n",
    "#         # create model\n",
    "#         hidden_size = trial.suggest_int(\"hidden_size\", *hidden_size_range, log=True)\n",
    "#         kwargs[\"loss\"] = copy.deepcopy(loss)\n",
    "#         model = RecurrentNetwork.from_dataset(\n",
    "#             train_dataloaders.dataset,\n",
    "#             dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
    "#             hidden_size=hidden_size,\n",
    "#             # hidden_continuous_size=trial.suggest_int(\n",
    "#             #     \"hidden_continuous_size\",\n",
    "#             #     hidden_continuous_size_range[0],\n",
    "#             #     min(hidden_continuous_size_range[1], hidden_size),\n",
    "#             #     log=True,\n",
    "#             # ),\n",
    "#             # attention_head_size=trial.suggest_int(\"attention_head_size\", *attention_head_size_range),\n",
    "#             log_interval=-1,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#         # find good learning rate\n",
    "#         if use_learning_rate_finder:\n",
    "#             lr_trainer = pl.Trainer(\n",
    "#                 gradient_clip_val=gradient_clip_val,\n",
    "#                 gpus=[0] if torch.cuda.is_available() else None,\n",
    "#                 logger=False,\n",
    "#                 enable_progress_bar=False,\n",
    "#                 enable_model_summary=False,\n",
    "#             )\n",
    "#             res = lr_trainer.tuner.lr_find(\n",
    "#                 model,\n",
    "#                 train_dataloaders=train_dataloaders,\n",
    "#                 val_dataloaders=val_dataloaders,\n",
    "#                 early_stop_threshold=10000,\n",
    "#                 min_lr=learning_rate_range[0],\n",
    "#                 num_training=100,\n",
    "#                 max_lr=learning_rate_range[1],\n",
    "#             )\n",
    "\n",
    "#             loss_finite = np.isfinite(res.results[\"loss\"])\n",
    "#             if loss_finite.sum() > 3:  # at least 3 valid values required for learning rate finder\n",
    "#                 lr_smoothed, loss_smoothed = sm.nonparametric.lowess(\n",
    "#                     np.asarray(res.results[\"loss\"])[loss_finite],\n",
    "#                     np.asarray(res.results[\"lr\"])[loss_finite],\n",
    "#                     frac=1.0 / 10.0,\n",
    "#                 )[min(loss_finite.sum() - 3, 10) : -1].T\n",
    "#                 optimal_idx = np.gradient(loss_smoothed).argmin()\n",
    "#                 optimal_lr = lr_smoothed[optimal_idx]\n",
    "#             else:\n",
    "#                 optimal_idx = np.asarray(res.results[\"loss\"]).argmin()\n",
    "#                 optimal_lr = res.results[\"lr\"][optimal_idx]\n",
    "#             optuna_logger.info(f\"Using learning rate of {optimal_lr:.3g}\")\n",
    "#             # add learning rate artificially\n",
    "#             model.hparams.learning_rate = trial.suggest_uniform(\"learning_rate\", optimal_lr, optimal_lr)\n",
    "#         else:\n",
    "#             model.hparams.learning_rate = trial.suggest_loguniform(\"learning_rate\", *learning_rate_range)\n",
    "\n",
    "#         # fit\n",
    "#         trainer.fit(model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders)\n",
    "\n",
    "#         # report result\n",
    "#         return metrics_callback.metrics[-1][\"val_loss\"].item()\n",
    "\n",
    "#     # setup optuna and run\n",
    "#     if study is None:\n",
    "#         study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "#     study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "#     return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from FileManager.dataManager import dataManager\n",
    "from AnalyzeTools.models import autoregressive_integrated_moving_average, linear_regression, support_vector_regression, random_forest, gradient_boosting\n",
    "from AnalyzeTools.prepare import data_split, model_eval, pathForSavingModels\n",
    "from AnalyzeTools.preprocess import preprocessData\n",
    "from AnalyzeTools.superModels import DEEPAR, TFT, RNN\n",
    "\n",
    "params_path = './Models/single'\n",
    "train_size = 0.8\n",
    "product_object = json.load(open(\"./File information.json\", \"r\", encoding='utf8'))\n",
    "\n",
    "all_experiments= []\n",
    "for product in product_object.keys():\n",
    "    for raw_file_name in  product_object[product].keys():\n",
    "        for product_type in product_object[product][raw_file_name]['product_types']:\n",
    "            for target in product_object[product][raw_file_name]['targets']:\n",
    "                all_experiments.append([product, raw_file_name, product_type, target])\n",
    "\n",
    "all_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -1\n",
    "experiment = all_experiments[n]\n",
    "product, raw_file_name, product_type, target = experiment\n",
    "print(f\"Product: {product}\\nRaw file name: {raw_file_name}\\nProduct_type: {product_type}\\ntarget: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, product_and_product_type, product_attribute = dataManager(raw_file_name, product, product_type, target)\n",
    "\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"No data!\")\n",
    "\n",
    "df, input_features = preprocessData(df, 'date', target)\n",
    "predictions_x_axis = df['date'][floor(len(df) * train_size):].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "data['time_idx'] = range(len(data))\n",
    "data['group'] = product\n",
    "\n",
    "training_cutoff = floor(len(data) * 0.8)\n",
    "\n",
    "max_prediction_length = 1\n",
    "max_encoder_length = 30 # 7, 14, 30, 60, 120\n",
    "batch_size = 64\n",
    "\n",
    "group = ['group']\n",
    "time_varying_known_categoricals = ['month', 'week']\n",
    "time_varying_unknown_categoricals = []\n",
    "time_varying_known_reals = ['time_idx']\n",
    "time_varying_unknown_reals = input_features + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLSTM\")\n",
    "lstm, val_dataloader = RNN(\n",
    "    data,\n",
    "    training_cutoff,\n",
    "    target,\n",
    "    group,\n",
    "    max_encoder_length,\n",
    "    max_prediction_length,\n",
    "    time_varying_known_categoricals,\n",
    "    time_varying_unknown_categoricals,\n",
    "    time_varying_known_reals,\n",
    "    batch_size,\n",
    "    pathForSavingModels(product_and_product_type, product_attribute, raw_file_name, 'LSTM'),\n",
    "    'LSTM'\n",
    ")\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "lstm_predictions = lstm.predict(val_dataloader)\n",
    "model_eval(actuals, lstm_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nDeepAR\")\n",
    "# deep_ar, val_dataloader = DEEPAR(\n",
    "#     data,\n",
    "#     training_cutoff,\n",
    "#     target,\n",
    "#     group,\n",
    "#     max_encoder_length,\n",
    "#     max_prediction_length,\n",
    "#     time_varying_known_categoricals,\n",
    "#     time_varying_unknown_categoricals,\n",
    "#     time_varying_known_reals,\n",
    "#     batch_size,\n",
    "#     pathForSavingModels(product_and_product_type, product_attribute, raw_file_name, 'DEEPAR'),\n",
    "# )\n",
    "\n",
    "# actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "# deepar_predictions = deep_ar.predict(val_dataloader)\n",
    "\n",
    "# model_eval(actuals, deepar_predictions, predictions_x_axis, stdout=True, vis=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f774c96c8c34c53ecd4c73b34542f198e825b7806220478caf5e39d6877a780"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
